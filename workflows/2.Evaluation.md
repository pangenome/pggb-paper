# Evaluation

Variables:

```shell
DIR_BASE=/lizardfs/guarracino/pggb-paper
DIR_GRAPHS=/lizardfs/guarracino/pggb-paper/graphs
DIR_GRAPHS_MC=/lizardfs/guarracino/pggb-paper/graphs_mc
DIR_EVALUATIONS=/lizardfs/guarracino/pggb-paper/evaluations
PATH_PANGENOMES_TSV=/lizardfs/guarracino/pggb-paper/data/pangenomes.tsv

ODGI=/home/guarracino/tools/odgi/bin/odgi-861b1c04f5622c5bb916a161c1abe812c213f1a5
RUN_GFA_2_EVALUATION=/lizardfs/guarracino/pggb-paper/scripts/gfa2evaluation.sh

# guix install python-scikit-learn python-seaborn
PANACUS=/home/guarracino/tools/panacus/target/release/panacus-bd492f54c05367d0fc5a2c3fb9bf23260ac8379e
PANACUS_VISUALIZE=/home/guarracino/tools/panacus/scripts/panacus-visualize.py

THREADS=48
```

SNVs evaluation with NUCMER:

```shell
mkdir -p $DIR_EVALUATIONS
cd $DIR_EVALUATIONS

sed '1d' $PATH_PANGENOMES_TSV | sort -k 4n | while read f p s n k G POA O REF; do
  seq 1 1 | while read c; do
    echo $f $p $s $n $k $G $POA $O $REF $c;

    out=$(basename "$f" .fa.gz)_p$p.s$s.n$n.k$k.G$(echo $G | tr ',' '-').P$POA.O$(echo $O | sed 's/\.//g')_$c
    PATH_GFA=$DIR_GRAPHS/$out/*.final.gfa
    PATH_GFA=$(eval echo $PATH_GFA) # force '*' expansion
    sbatch -c $THREADS -p workers -J $(basename "$f" .fa.gz) --wrap "hostname; cd /scratch; $RUN_GFA_2_EVALUATION $PATH_GFA $REF $DIR_EVALUATIONS/$out $THREADS;"
  done
done

cd $DIR_EVALUATIONS

for type in "" "waved."; do
  echo $type

  # All results
  ((echo -n replicate"\t"dataset"\t"; cat */statistics*.tsv | head -n 1 | tr -d '\n'; echo -e "\tpggb.evaluated"); ls */statistics.haplo.${type}tsv | while read f; do
    name=$(echo $f | cut -f 1 -d '/');
    sed '1d' $f | awk -v name="$name" -v OFS='\t' 'NF>=12 { print(name, $0) }' | awk -v OFS='\t' '{print(substr($1, length($1), 1),$0,$4+$5)}';
  done) | sort -k 1,1n -k 2,2 > nucmer-based_evaluation.${type}tsv

  # Average results
  # GCA_028009825.1 and GCA_028009825.2 are identical, so they create nan in the evaluation
  cat nucmer-based_evaluation.${type}tsv |
    awk '
    BEGIN { FS=OFS="\t" }
    NR>1 {
        count[($1 FS $2)]++
        for (i=4; i<=NF; i++) {  # Start from the 4th column
            sum[($1 FS $2 FS i)] += $i
        }
    }
    END {
        print "replicate", "dataset", "tp.baseline", "tp.call", "fp", "fn", "precision", "recall", "f1.score", "nucmer.tot", "pggb.tot", "nucmer.ratio", "pggb.ratio", "pggb.evaluated"
        for (key in count) {
            printf "%s", key
            for (i=4; i<=15; i++) {  # Adjust the indices here as well
                printf "%s%.6f", OFS, sum[key FS i] / count[key]
            }
            print ""
        }
    }' | sort -k 1,1n -k 2,2 > nucmer-based_evaluation.${type}mean.tsv
done
```

SNVs and INDELs evaluation:

tomato/athaliana (for hsapiens, refer to the HPRC paper?)

```shell
# Call variants
cd $DIR_GRAPHS

sed '1d' $PATH_PANGENOMES_TSV | sort -k 4n | grep 'hsapiens\|tomato\|athaliana' | while read f p s n k G POA O REF; do
  seq 1 1 | while read c; do
    if [[ $f == *"hsapiens"* ]]; then
      REF="grch38" # hsapiens variants are called w.r.t. grch38
    fi
    echo $f $p $s $n $k $G $POA $O $REF $c;

    out=$(basename "$f" .fa.gz)_p$p.s$s.n$n.k$k.G$(echo $G | tr ',' '-').P$POA.O$(echo $O | sed 's/\.//g')_$c
    PATH_GFA=$DIR_GRAPHS/$out/*.final.gfa
    PATH_GFA=$(eval echo $PATH_GFA) # force '*' expansion
    PREFIX=$(echo $PATH_GFA | sed 's/.gfa//g')

    sbatch -c $THREADS -p allnodes --job-name vg-$out --wrap "hostname; cd $DIR_GRAPHS/$out; vg deconstruct -P $REF -e -a -t $THREADS $PATH_GFA | bgzip -@ $THREADS -l 9 > $PREFIX.$REF.vcf.gz; vcfbub -l 0 -a 10000 --input $PREFIX.$REF.vcf.gz | vcfwave -I 1000 -t $THREADS > $PREFIX.$REF.decomposed.tmp.vcf; bcftools annotate -x INFO/TYPE $PREFIX.$REF.decomposed.tmp.vcf | awk '\$5 != \".\"' | bgzip -@ $THREADS -l 9 > $PREFIX.$REF.decomposed.vcf.gz; tabix $PREFIX.$REF.decomposed.vcf.gz; rm $PREFIX.$REF.decomposed.tmp.vcf; zcat $PREFIX.$REF.decomposed.vcf.gz | bcftools view -a -Ou | bcftools norm -m - --threads $THREADS -Ou | bcftools sort -m 40G -T /scratch/bcftools-sort.XXXXXX -Ou | bcftools norm -d exact -Ov --threads $THREADS | python3 /lizardfs/guarracino/rat_32samples/SV/fix_svtype_vcf.py | bgzip -@ $THREADS -l 9 > $PREFIX.$REF.decomposed.norm.vcf.gz; tabix $PREFIX.$REF.decomposed.norm.vcf.gz"
  done
done


# H. sapiens
cut -f 1 /lizardfs/erikg/HPRC/year1v2genbank/evaluation/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna -d ' ' | sed 's/>chr/>grch38#1#chr/g' > $DIR_BASE/vcfs/hsapiens/grch38.fa
samtools faidx $DIR_BASE/vcfs/hsapiens/grch38.fa
rtg format -o $DIR_BASE/vcfs/hsapiens/grch38.fa.sdf $DIR_BASE/vcfs/hsapiens/grch38.fa

# VCF files (from https://s3-us-west-2.amazonaws.com/human-pangenomics/index.html?prefix=submissions/B581EBA7-8BDE-4C7C-9DEA-78B99A051155--Yale_HPP_Year1_Variant_Calls/)
mkdir -p $DIR_BASE/vcfs/hsapiens
cd $DIR_BASE/vcfs/hsapiens
cut -f 2 $DIR_BASE/data/HPRC.deepvariant-vcf.urls.tsv | parallel -j 4 'wget -q {} && echo got {}'
ls $DIR_BASE/vcfs/hsapiens/*vcf.gz | while read VCF; do
  echo $VCF
  NAME=$(basename $VCF .vcf.gz)
  zcat $VCF | sed 's/chr/grch38#1#chr/g' | bgzip -@ 48 -l 9 > $NAME.pansn.vcf.gz
  rm $VCF
  tabix $NAME.pansn.vcf.gz
done

# Confident regions (from https://s3-us-west-2.amazonaws.com/human-pangenomics/index.html?prefix=working/HPRC/HG00438/assemblies/year1_freeze_assembly_v2/assembly_qc/dipcall_v0.2/)
cut -f 2 $DIR_BASE/data/HPRC.dipcall-confident-regions.urls.tsv | parallel -j 4 'wget -q {} && echo got {}'
ls $DIR_BASE/vcfs/hsapiens/*bed | while read BED; do
  echo $BED
  NAME=$(basename $BED .bed)
  cat $BED | sed 's/chr/grch38#1#chr/g' | sort -V | bgzip -@ 48 -l 9 > $NAME.confident-yes.pansn.bed.gz
  rm $BED
  tabix -p bed $NAME.confident-yes.pansn.bed.gz
done

# Stratification (from https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v2.0/GRCh38/union/)
wget -c https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v2.0/GRCh38/union/GRCh38_notinalldifficultregions.bed.gz # easy regions
wget -c https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v2.0/GRCh38/union/GRCh38_alllowmapandsegdupregions.bed.gz # low map and SegDup
wget -c https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v2.0/GRCh38/union/GRCh38_alldifficultregions.bed.gz # hard regions (low map and SegDup included)
for BED in GRCh38_notinalldifficultregions.bed.gz GRCh38_alllowmapandsegdupregions.bed.gz GRCh38_alldifficultregions.bed.gz; do
  zcat $BED | sed '1d' | sed 's/^chr/grch38#1#chr/g' | bgzip -@ 48 -l 9 > x
  mv x $BED
  tabix -p bed $BED
done

cd $DIR_BASE/evaluations_vcfeval
ls $DIR_GRAPHS/hsapiens*/*decomposed.vcf.gz | while read VCF; do
  NAME="$(dirname $VCF | rev | cut -f 1 -d '/' | rev)"
  echo $NAME

  sbatch -c 48 -p tux --job-name $NAME --wrap "hostname; bash $DIR_BASE/scripts/evaluation.speciesN.sh \
    $VCF \
    grch38#1#chr6 \
    $NAME \
    $DIR_BASE/evaluations_vcfeval \
    $DIR_BASE/vcfs/hsapiens/grch38.fa \
    $DIR_BASE/vcfs/hsapiens/grch38.fa.sdf \
    $DIR_BASE/vcfs/hsapiens \
    .GRCh38_no_alt.deepvariant.pansn.vcf.gz \
    "":"" \
    ".easy":$DIR_BASE/vcfs/hsapiens/GRCh38_notinalldifficultregions.bed.gz \
    ".hard":$DIR_BASE/vcfs/hsapiens/GRCh38_alldifficultregions.bed.gz #\
    #".confident-yes":$DIR_BASE/vcfs/hsapiens/$SAMPLE.f1_assembly_v2.dip.confident-yes.pansn.bed.gz \
    #".lowmap-segdup":$DIR_BASE/vcfs/hsapiens/GRCh38_alllowmapandsegdupregions.bed.gz"
done


# Tomato
mkdir -p $DIR_BASE/vcfs/tomato
cd $DIR_BASE/vcfs/tomato

zcat $DIR_BASE/assemblies/tomato/SL5.fa.gz > $DIR_BASE/vcfs/tomato/SL5.fa
samtools faidx $DIR_BASE/vcfs/tomato/SL5.fa
rtg format -o $DIR_BASE/vcfs/tomato/SL5.fa.sdf $DIR_BASE/vcfs/tomato/SL5.fa

# Transfer VCF files from Google Drive to /lizardfs/guarracino/pggb-paper/vcfs/tomato
# Fix header and apply PanSN
# Clean
rm $DIR_BASE/vcfs/tomato/*hifi.vcf.gz*

# Stratification
wget -c http://solomics.agis.org.cn/tomato/ftp/repeat/SL5.0.repeat.gff3.gz
zgrep -v "^#" SL5.0.repeat.gff3.gz | cut -f 1,4,5 | sort -k1,1V -k2,2n | bedtools merge -d 100 | sed 's/^/SL5#1#chr/g' | bedtools sort | bgzip -@ 48 -l 9 > SL5.hard.bed.gz # TE
bedtools subtract -a <(awk -v OFS='\t' '{print($1,"0",$2)}' $DIR_BASE/assemblies/tomato/SL5.fa.gz.fai) -b SL5.hard.bed.gz | bgzip -@ 48 -l 9 > SL5.easy.bed.gz

cd $DIR_BASE/evaluations_vcfeval
ls $DIR_GRAPHS/tomato*/*decomposed.vcf.gz | while read VCF; do
  NAME="$(dirname $VCF | rev | cut -f 1 -d '/' | rev)"
  CHR=$(echo $NAME | cut -f 2 -d '.' | cut -f 1 -d '_')
  echo $NAME

  bash $DIR_BASE/scripts/evaluation.speciesN.sh \
    $VCF \
    SL5#1#$CHR \
    $NAME \
    $DIR_BASE/evaluations_vcfeval \
    $DIR_BASE/vcfs/tomato/SL5.fa \
    $DIR_BASE/vcfs/tomato/SL5.fa.sdf \
    $DIR_BASE/vcfs/tomato \
    .hifi.pansn.vcf.gz \
    "":"" \
    ".easy":$DIR_BASE/vcfs/tomato/SL5.easy.bed.gz \
    ".hard":$DIR_BASE/vcfs/tomato/SL5.hard.bed.gz
done


# A. thaliana
mkdir -p $DIR_BASE/vcfs/athaliana
cd $DIR_BASE/vcfs/athaliana

tabix -p bed $DIR_BASE/data/Ath.easy.region.bed.gz
tabix -p bed $DIR_BASE/data/Ath.hard.region.bed.gz

bcftools concat panCEN.dv.Chr*.vcf.gz | sed "s/Chr/Col-CC-v2#1#chr/g" | bgzip -@ 48 -l 9 > All.hifi.pansn.vcf.gz && tabix All.hifi.pansn.vcf.gz
rm panCEN.dv.Chr*.vcf.gz 

zgrep '^#CHROM' All.hifi.pansn.vcf.gz -m 1 | cut -f 10- | tr '\t' '\n' | while read SAMPLE; do
  echo $SAMPLE
  bcftools view --threads 48 -s $SAMPLE All.hifi.pansn.vcf.gz | bcftools norm -m -any -O u - | bcftools view --threads 48 -e 'GT="ref" | GT="0/." | GT="./0" | GT="./."|STRLEN(ALT)-STRLEN(REF)>=50 | STRLEN(ALT)-STRLEN(REF)<=-50|QUAL<30|DP<5|DP>400' -O z -l 9 -o $SAMPLE.hifi.pansn.vcf.gz
  tabix $SAMPLE.hifi.pansn.vcf.gz
done

################################################################################################################
################################################################################################################
################################################################################################################

SED_CMD=""
while IFS= read -r line; do
    search=$(echo "$line" | awk '{print $1}')
    replace=$(echo "$line" | awk '{print $2}')
    SED_CMD+="s/$search/$replace/g;"
done < $DIR_BASE/data/athaliana82.acc2name.tsv

zcat $DIR_BASE/assemblies/athaliana/GCA_028009825.2.fasta.gz | sed "$SED_CMD" | sed -e "s/CP116280.1/chr1/g" -e "s/CP116281.2/chr2/g" -e "s/CP116282.1/chr3/g" -e "s/CP116283.2/chr4/g" -e "s/CP116284.1/chr5/g" > $DIR_BASE/vcfs/athaliana/Col-CC-v2.fa
samtools faidx $DIR_BASE/vcfs/athaliana/Col-CC-v2.fa
rtg format -o $DIR_BASE/vcfs/athaliana/Col-CC-v2.fa.sdf $DIR_BASE/vcfs/athaliana/Col-CC-v2.fa

ls $DIR_GRAPHS/athaliana*/*decomposed.vcf.gz | while read VCF; do
  echo $VCF

  PREFIX=$(echo $VCF | sed 's/.vcf.gz//g')
  zcat $VCF | sed "$SED_CMD" | sed -e "s/CP116280.1/chr1/g" -e "s/CP116281.2/chr2/g" -e "s/CP116282.1/chr3/g" -e "s/CP116283.2/chr4/g" -e "s/CP116284.1/chr5/g" | bcftools sort -m 40G -T /scratch/bcftools-sort.XXXXXX | bgzip -@ 48 -l 9 > $PREFIX.renamed.vcf.gz
  tabix $PREFIX.renamed.vcf.gz
done

cd $DIR_BASE/evaluations_vcfeval
ls $DIR_GRAPHS/athaliana*/*decomposed.renamed.vcf.gz | while read VCF; do
  NAME="$(dirname $VCF | rev | cut -f 1 -d '/' | rev)"
  CHR=$(echo $NAME | cut -f 2 -d '.' | cut -f 1 -d '_')
  echo $NAME

  sbatch -c 48 -p tux --job-name $NAME --wrap "hostname; bash $DIR_BASE/scripts/evaluation.speciesN.sh \
    $VCF \
    Col-CC-v2#1#$CHR \
    $NAME \
    $DIR_BASE/evaluations_vcfeval \
    $DIR_BASE/vcfs/athaliana/Col-CC-v2.fa \
    $DIR_BASE/vcfs/athaliana/Col-CC-v2.fa.sdf \
    $DIR_BASE/vcfs/athaliana \
    .hifi.pansn.vcf.gz \
    "":"" \
    ".easy":$DIR_BASE/data/Ath.easy.region.bed.gz \
    ".hard":$DIR_BASE/data/Ath.hard.region.bed.gz"
done
```

Compression:

```shell
cd $DIR_GRAPHS

(echo "dataset graph.len pangenome.len compression.ratio" | tr ' ' '\t'; ls $DIR_GRAPHS/*/*.final.og | while read GRAPH; do
  DIR_PARENT="$(dirname $GRAPH)"
  NAME=$(echo $GRAPH | rev | cut -f 2 -d '/' | rev)
  PREFIX=$DIR_PARENT/$NAME

  GRAPH_LEN=$($ODGI stats -i $GRAPH -S | tail -n 1 | cut -f 1)
  PANGENOME_LEN=$(cat $(grep Command $DIR_PARENT/*.log | cut -f 4 -d ' ').fai | awk '{sum+=$2}END{print(sum)}')
  echo $NAME $GRAPH_LEN $PANGENOME_LEN | awk -v OFS='\t' '{print($0,$3/$2)}'
done) > compression.stats.tsv
```

Openness:

```shell
ls $DIR_GRAPHS/*/*gfa | grep athaliana -v | while read GRAPH; do
  DIR_PARENT="$(dirname $GRAPH)"
  NAME=$(echo $GRAPH | rev | cut -f 2 -d '/' | rev)
  PREFIX=$DIR_PARENT/$NAME
  #NUM=$(echo $NAME | cut -f 1 -d '.' | grep -oP '\d+$')
  echo $PREFIX
  sbatch -c 48 -p tux --job-name panacus-$NAME --wrap "hostname; $PANACUS histgrowth $GRAPH -c bp -q 0,1,0.5,0.1 --groupby-sample -t 48 > $PREFIX.histgrowth.tsv; $PANACUS_VISUALIZE --estimate_growth_params $PREFIX.histgrowth.tsv -s 15 10 > $PREFIX.histgrowth.pdf"
done
```

Visualization by merging contigs by haplotype:

```shell
cd $DIR_BASE

ls $DIR_GRAPHS/*/*.final.og | while read GRAPH; do
  DIR_PARENT="$(dirname $GRAPH)"
  NAME=$(echo $GRAPH | rev | cut -f 2 -d '/' | rev)
  PREFIX=$DIR_PARENT/$NAME
  echo $NAME

  if [[ ! -s $PREFIX.prefixes.txt ]]; then
    $ODGI paths -i $GRAPH -L | cut -f 1,2 -d '#' | sort | uniq > $PREFIX.prefixes.txt
  fi
  
  sbatch -c 8 -p workers -J $NAME-N  --wrap "hostname; $ODGI viz -i $GRAPH -o $PREFIX.merged_by_haplotype.N.png  -x 1000 -y 500 -a 10 -N  -I Consensus_ -M $PREFIX.prefixes.txt"
  sbatch -c 8 -p workers -J $NAME-z  --wrap "hostname; $ODGI viz -i $GRAPH -o $PREFIX.merged_by_haplotype.z.png  -x 1000 -y 500 -a 10 -z  -I Consensus_ -M $PREFIX.prefixes.txt"
  sbatch -c 8 -p workers -J $NAME-du --wrap "hostname; $ODGI viz -i $GRAPH -o $PREFIX.merged_by_haplotype.du.png -x 1000 -y 500 -a 10 -du -I Consensus_ -M $PREFIX.prefixes.txt"
done
```
